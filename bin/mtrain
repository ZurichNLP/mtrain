#!/usr/bin/env python3

import logging
import argparse
import sys
import os

from mtrain.training import Training
from mtrain.constants import *

LOGGING_LEVELS = {
    "DEBUG": logging.DEBUG,
    "INFO": logging.INFO,
    "WARNING": logging.WARNING,
    "ERROR": logging.ERROR,
    "CRITICAL": logging.CRITICAL,
}

def get_argument_parser():
    parser = argparse.ArgumentParser()
    parser.description = "Trains a Moses-based machine translation system."
    parser.add_argument(
        "basepath",
        type=str,
        help="common path/file prefix of the training corpus' source and " +
        "target side, e.g., `/foo/bar/training_corpus`"
    )
    parser.add_argument(
        "src_lang",
        type=str,
        help="source language code: valid choices are `" +
            "`, `".join([lang for lang in sorted(MOSES_TOKENIZER_LANG_CODES.keys())]) +
            "`",
        choices=MOSES_TOKENIZER_LANG_CODES.keys(),
        metavar='src_lang' #overrides ugnly double-listing of available choices in --help
    )
    parser.add_argument(
        "trg_lang",
        type=str,
        help="target language code: same valid choices as in src_lang",
        choices=MOSES_TOKENIZER_LANG_CODES.keys(),
        metavar='trg_lang' #overrides ugnly double-listing of available choices in --help
    )
    parser.add_argument(
        "-o", "--output_dir",
        type=str,
        help="target directory for all output. The curent working directory " +
            "($PWD) is used by default.",
        default=os.getcwd()
    )
    parser.add_argument(
        "-c", "--caser",
        type=str,
        help="casing strategy: " +
            "; ".join(["`%s`: %s" % (name, descr) for name, descr in CASING_STRATEGIES.items()]),
        choices=CASING_STRATEGIES.keys(),
        default="recasing"
    )
    parser.add_argument(
        "-t", "--tune",
        help="enable tuning. If an integer is provided, the given number of " +
            "segments will be randomly taken from the training corpus " +
            "(basepath). Alternatively, the basepath to a separate tuning " +
            "corpus can be provided. Examples: `2000`, `/foo/bar/tuning_corpus`"
    )
    parser.add_argument(
        "-e", "--eval",
        help="enable evaluation. If an integer is provided, the given number " +
            "of segments will be randomly taken from the training corpus " +
            "(basepath). Alternatively, the basepath to a separate evaluation " +
            "corpus can be provided. Examples: `2000`, `/foo/bar/eval_corpus`"
    )
    parser.add_argument(
        "--tokenize_external",
        help="tokenize external tuning and evaluation corpora. Don't use if " +
            "the external files provided in --tune and --eval are already " +
            "tokenized.",
        default=False,
        action='store_true'
    )
    parser.add_argument(
        "-n", "--n_gram_order",
        help="the language model's n-gram order",
        type=int,
        default=5
    )
    parser.add_argument(
        "--min_tokens",
        help="the minimum number of tokens per segments; segments with less " +
            "tokens will be discarded",
        type=int,
        default=1
    )
    parser.add_argument(
        "--max_tokens",
        help="the maximum number of tokens per segments; segments with more " +
            "tokens will be discarded",
        type=int,
        default=80
    )
    parser.add_argument(
        "--threads",
        help="the number of threads to be used at most",
        type=int,
        default=8
    )
    parser.add_argument(
        "--temp_dir",
        help="directory for temporary files created during trainig",
        default="/tmp"
    )
    parser.add_argument(
        "--keep_uncompressed_models",
        help="do not delete uncompressed models created during training",
        action='store_true'
    )
    parser.add_argument(
        "--logging",
        help="logging level in stdout",
        choices=LOGGING_LEVELS.keys(),
        default="INFO"
    )
    parser.add_argument(
        "--stdout",
        help="write logs to standard output rather than the default log file.",
        action='store_true'
    )
    return parser

def main():
    parser = get_argument_parser()
    args = parser.parse_args()
    # parse arguments
    if args.tune:
        try:
            args.tune = int(args.tune)
        except ValueError:
            pass # string is fine; means path to external tuning corpus was provided
    if args.eval:
        try:
            args.eval = int(args.eval)
        except ValueError:
            pass # string is fine; means path to external evalution corpus was provided
    # initialize logging
    # log all events to file
    logging.basicConfig(
        filename=args.output_dir + os.sep + 'training.log',
        level=logging.DEBUG,
        format='%(asctime)s - mtrain - %(levelname)s - %(message)s'
    )
    # log WARNING and above (or as specified by user) to stdout
    console = logging.StreamHandler()
    console.setLevel(LOGGING_LEVELS[args.logging])
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger("").addHandler(console)
    # training
    training = Training(
        basepath=args.output_dir,
        src_lang=args.src_lang,
        trg_lang=args.trg_lang,
        casing_strategy=args.caser,
        tuning=args.tune,
        evaluation=args.eval
    )
    training.preprocess(
        base_corpus_path=args.basepath,
        min_tokens=args.min_tokens,
        max_tokens=args.max_tokens,
        tokenize_external=args.tokenize_external
    )
    if args.caser == TRUECASING:
        training.train_truecaser()
        training.truecase()
    elif args.caser == RECASING:
        training.train_recaser(
            args.threads,
            args.temp_dir,
            args.keep_uncompressed_models
        )
    training.train_engine(
        n=args.n_gram_order,
        alignment='grow-diag-final-and', # todo: make changeable
        max_phrase_length=7, # todo: make changeable
        reordering='msd-bidirectional-fe', # todo: make changeable
        num_threads=args.threads,
        path_temp_files=args.temp_dir,
        keep_uncompressed=args.keep_uncompressed_models
    )
    if args.tune:
        training.tune(args.threads)
    training.write_final_ini()

if __name__ == '__main__':
    main()
